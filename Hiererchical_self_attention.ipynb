{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hiererchical self attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNe8ms2eGEdA78w16eEJChm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saif-mahmud/hierarchical-attention-HAR/blob/master/Hiererchical_self_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FB-eyA9BQCf7",
        "colab_type": "code",
        "outputId": "77ecca14-f411-4876-f1b5-664a6e2ce30b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGlJDgDrZrHt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, position, d_model, include_dropout=True, dropout_rate=0.2):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "        self.include_dropout = include_dropout\n",
        "        if include_dropout:\n",
        "            self.dropout = tf.keras.layers.Dropout(rate=dropout_rate)\n",
        "\n",
        "    def get_angles(self, position, i, d_model):\n",
        "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "        return position * angles\n",
        "\n",
        "    def positional_encoding(self, position, d_model):\n",
        "        angle_rads = self.get_angles(\n",
        "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
        "            d_model=d_model)\n",
        "        # apply sin to even index in the array\n",
        "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "        # apply cos to odd index in the array\n",
        "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "\n",
        "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
        "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "        return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "    def call(self, x):\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x = x + self.pos_encoding[:, :tf.shape(x)[1], :]\n",
        "        if self.include_dropout:\n",
        "            x = self.dropout(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKVIThNRcX6L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LearnedPositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_positions, d_model, include_dropout=True, dropout_rate=0.2):\n",
        "        super(LearnedPositionalEncoding, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.pos_encoding = tf.keras.layers.Embedding(num_positions, d_model)\n",
        "        self.include_dropout = include_dropout\n",
        "        if include_dropout:\n",
        "            self.dropout = tf.keras.layers.Dropout(rate=dropout_rate)\n",
        "\n",
        "    def call(self, x):\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x = x + self.pos_encoding(tf.expand_dims(tf.range(tf.shape(x)[1]), axis=0))\n",
        "        if self.include_dropout:\n",
        "            x = self.dropout(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYT3Tbh4QhnL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"Calculate the attention weights.\n",
        "    q, k, v must have matching leading dimensions.\n",
        "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "    The mask has different shapes depending on its type(padding or look ahead) \n",
        "    but it must be broadcastable for addition.\n",
        "\n",
        "    Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable \n",
        "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "    output, attention_weights\n",
        "    \"\"\"\n",
        "\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "    # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model,use_bias=False)\n",
        "        self.wk = tf.keras.layers.Dense(d_model,use_bias=True)\n",
        "        self.wv = tf.keras.layers.Dense(d_model,use_bias=True)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention, \n",
        "                                        (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "            \n",
        "        return output, attention_weights\n",
        "\n",
        "\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "    ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odawothQxCMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AggregateAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(AggregateAttention, self).__init__()\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.d_model = d_model\n",
        "        self.query = self.add_weight(\"learned_query\",\n",
        "                                     shape=[1, 1, self.d_model],\n",
        "                                     initializer=tf.keras.initializers.Orthogonal())  \n",
        "    \n",
        "    def call(self, v, k):\n",
        "        batched_query = tf.tile(self.query, [tf.shape(v)[0],1,1])\n",
        "        output, attention_weights = self.mha(v, k, batched_query, mask=None)\n",
        "        output = tf.squeeze(output, axis=1)\n",
        "        attention_weights = tf.squeeze(attention_weights, axis=2)\n",
        "        return output, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_YDHy_Pbg97",
        "colab_type": "text"
      },
      "source": [
        "Testing the shapes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbC6cGy0atD0",
        "colab_type": "code",
        "outputId": "0c1901f8-e676-4f52-da4c-86accd00e31e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "temp_agg_attn = AggregateAttention(d_model=32, num_heads=2)\n",
        "\n",
        "y = tf.random.uniform((128, 10, 32))  # (batch_size, sequence_len, d_model)\n",
        "\n",
        "out, attn = temp_agg_attn(y, k=y)\n",
        "\n",
        "out.shape, attn.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([128, 32]), TensorShape([128, 2, 10]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syJtH2GUhVTF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AggregateAttentionBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, expand_dim_factor=1, rate=0.1):\n",
        "        super(AggregateAttentionBlock, self).__init__()\n",
        "        self.aga = AggregateAttention(d_model*expand_dim_factor, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model*expand_dim_factor, dff*expand_dim_factor)\n",
        "\n",
        "        self.layernorm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training):\n",
        "        attn_output, attn_scores = self.aga(x, x)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        \n",
        "        ffn_output = self.ffn(attn_output)  # (batch_size, d_model)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out = self.layernorm(ffn_output + attn_output)  # (batch_size, d_model)\n",
        "        return out, attn_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZO3ZSJt9jL8p",
        "colab_type": "code",
        "outputId": "f3747c17-8222-4e79-93f4-5d4dc43e5858",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "temp_agg_attnblock = AggregateAttentionBlock(d_model=32, num_heads=4, dff=128)\n",
        "\n",
        "y = tf.random.uniform((16, 10, 32))  # (batch_size, sequence_len, d_model)\n",
        "\n",
        "block_out, attn = temp_agg_attnblock(y, True)\n",
        "\n",
        "block_out.shape, attn.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([16, 32]), TensorShape([16, 4, 10]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tg1dh_ajyO8l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SelfAttentionBlock(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(SelfAttentionBlock, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "  def call(self, x, training, mask=None):\n",
        "\n",
        "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "    \n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "    \n",
        "    return out2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXotyreEfoNY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, n_timesteps, d_model, num_heads, dff, num_sa_blocks=1, add_pe=True, pe_type = 'learned', expand_dim_factor=1, dropout_rate=0.1):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        self.add_pe = add_pe\n",
        "        if self.add_pe:\n",
        "            if pe_type == 'fixed': \n",
        "                self.pe = PositionalEncoding(n_timesteps, d_model, dropout_rate=dropout_rate)\n",
        "            else:\n",
        "                self.pe = LearnedPositionalEncoding(n_timesteps, d_model, dropout_rate=dropout_rate)\n",
        "        self.pe = PositionalEncoding(n_timesteps, d_model)\n",
        "        self.self_attn_blocks = tf.keras.Sequential(\n",
        "            [SelfAttentionBlock(d_model, num_heads, dff, dropout_rate) for _ in range(num_sa_blocks)]\n",
        "            )\n",
        "        self.attn_aggr = AggregateAttentionBlock(d_model, num_heads, dff, expand_dim_factor, dropout_rate)\n",
        "\n",
        "    def call(self, x):\n",
        "        if self.add_pe:\n",
        "            x = self.pe(x)\n",
        "        x = self.self_attn_blocks(x)\n",
        "        x, _  = self.attn_aggr(x)\n",
        "        return x\n",
        "\n",
        "    def compute_output_shape(x, input_shape):\n",
        "        return tf.TensorShape([input_shape[0], input_shape[-1]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iH3IVQo8eLYb",
        "colab_type": "code",
        "outputId": "682cf48d-6dee-47a8-c60c-7626d5230001",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "temp_encoder = EncoderBlock(20, d_model=32, num_heads=4, dff=128, num_sa_blocks=2)\n",
        "\n",
        "y = tf.random.uniform((128, 10, 32))  # (batch_size, sequence_len, d_model)\n",
        "\n",
        "block_out = temp_encoder(y)\n",
        "\n",
        "block_out.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([128, 32])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHIYaBr4ZvCN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HSAEncoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, n_window, n_timesteps, d_model, num_heads, dff, expand_dim_factor=1, dropout_rate=0.1):\n",
        "        super(HSAEncoder, self).__init__()\n",
        "        self.n_window = n_window\n",
        "        self.d_model = d_model\n",
        "        self.n_timesteps = n_timesteps\n",
        "        self.expand_dim_factor = expand_dim_factor\n",
        "        self.window_encoder = EncoderBlock(n_timesteps, d_model, num_heads,dff,\n",
        "                                           num_sa_blocks=2, add_pe=True,\n",
        "                                           pe_type = 'fixed',\n",
        "                                           expand_dim_factor = expand_dim_factor, \n",
        "                                           dropout_rate=dropout_rate)\n",
        "        self.session_encoder = EncoderBlock(n_window, d_model*expand_dim_factor,\n",
        "                                            num_heads,dff*expand_dim_factor,\n",
        "                                            num_sa_blocks=1, add_pe=True,\n",
        "                                            pe_type = 'learned',\n",
        "                                            expand_dim_factor = expand_dim_factor,\n",
        "                                            dropout_rate=dropout_rate)\n",
        "     \n",
        "        \n",
        "    def call(self, x):\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        x = tf.reshape(x, (-1, self.n_timesteps, self.d_model)) # all sessions in batch dim\n",
        "        x = self.window_encoder(x)\n",
        "        x = tf.reshape(x, (batch_size, -1, self.d_model*self.expand_dim_factor))\n",
        "        x = self.session_encoder(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "    def compute_output_shape(x, input_shape):\n",
        "        return tf.TensorShape([input_shape, self.d_model*self.expand_dim_factor**2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqceHPDCjuys",
        "colab_type": "code",
        "outputId": "aadc791c-a3ce-484f-cc83-72651c431237",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "temp_hier_attn = HSAEncoder(n_window=30, n_timesteps=100, d_model=64, num_heads=4, dff=128)\n",
        "\n",
        "y = tf.random.uniform((16, 5, 100, 64))  # (batch_size, no_of_window, sequence_len, d_model)\n",
        "\n",
        "block_out = temp_hier_attn(y)\n",
        "\n",
        "block_out.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([16, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nTBFpugUyus",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLPClassifier(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_classes, hidden_layers = [256]):\n",
        "        super(MLPClassifier, self).__init__()\n",
        "        self.layers = tf.keras.Sequential(\n",
        "            [tf.keras.layers.Dense(num_units, activation='relu') for num_units in hidden_layers]\n",
        "        )\n",
        "        self.final_layer = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "        self.dropout = tf.keras.layers.Dropout(0.3)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.layers(x)\n",
        "        x = self.dropout(x)\n",
        "        return self.final_layer(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JymbZtqkjAC3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HARModel(tf.keras.Model):\n",
        "    def __init__(self, num_classes, n_window, n_timesteps, d_model, num_heads, dff,expand_dim_factor=1, dropout_rate=0.3):\n",
        "        super(HARModel, self).__init__()\n",
        "        self.data_transform = tf.keras.layers.TimeDistributed(tf.keras.layers.Conv1D(d_model, 1, activation='relu'))\n",
        "        self.encoder = HSAEncoder(n_window, n_timesteps, d_model, num_heads, dff,expand_dim_factor, dropout_rate)\n",
        "        self.classifier = MLPClassifier(num_classes)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.data_transform(x)\n",
        "        x = self.encoder(x)\n",
        "        return self.classifier(x)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}