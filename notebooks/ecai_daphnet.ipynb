{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from preprocessing.sliding_window import create_windowed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/hariub/data/HAR/processed/clean_daphnet_data.csv'\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Ankle_acc_x</th>\n",
       "      <th>Ankle_acc_y</th>\n",
       "      <th>Ankle_acc_z</th>\n",
       "      <th>Thigh_acc_x</th>\n",
       "      <th>Thigh_acc_y</th>\n",
       "      <th>Thigh_acc_z</th>\n",
       "      <th>Trunk_acc_x</th>\n",
       "      <th>Trunk_acc_y</th>\n",
       "      <th>Trunk_acc_z</th>\n",
       "      <th>Label</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Run</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>-970.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>-970.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>-960.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>-960.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>78.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>-960.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time  Ankle_acc_x  Ankle_acc_y  Ankle_acc_z  Thigh_acc_x  Thigh_acc_y  \\\n",
       "0  15.0         70.0         39.0       -970.0          0.0          0.0   \n",
       "1  31.0         70.0         39.0       -970.0          0.0          0.0   \n",
       "2  46.0         60.0         49.0       -960.0          0.0          0.0   \n",
       "3  62.0         60.0         49.0       -960.0          0.0          0.0   \n",
       "4  78.0         50.0         39.0       -960.0          0.0          0.0   \n",
       "\n",
       "   Thigh_acc_z  Trunk_acc_x  Trunk_acc_y  Trunk_acc_z  Label  Subject  Run  \n",
       "0          0.0          0.0          0.0          0.0    0.0        1    1  \n",
       "1          0.0          0.0          0.0          0.0    0.0        1    1  \n",
       "2          0.0          0.0          0.0          0.0    0.0        1    1  \n",
       "3          0.0          0.0          0.0          0.0    0.0        1    1  \n",
       "4          0.0          0.0          0.0          0.0    0.0        1    1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = ['Ankle_acc_x', 'Ankle_acc_y', 'Ankle_acc_z', 'Thigh_acc_x','Thigh_acc_y', 'Thigh_acc_z', 'Trunk_acc_x', 'Trunk_acc_y','Trunk_acc_z']\n",
    "LABEL = ['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0)\n",
    "scaler = StandardScaler()\n",
    "df[FEATURES] = scaler.fit_transform(df[FEATURES])\n",
    "\n",
    "df = df[df['Label'] != 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = yaml.load(open('../configs/metadata.yaml', mode='r'), Loader=yaml.FullLoader)\n",
    "\n",
    "metadata = yaml.load(open('../configs/metadata.yaml', mode='r'), Loader=yaml.FullLoader)['daphnet_preprocess']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[(df['Subject'] != 2) & (df['Subject'] != 9)]\n",
    "test_df = df[df['Subject'] == 2]\n",
    "val_df = df[df['Subject'] == 9]\n",
    "\n",
    "SLIDING_WINDOW_LENGTH = metadata['sliding_win_len']\n",
    "SLIDING_WINDOW_STEP = metadata['sliding_win_stride']\n",
    "N_WINDOW, N_TIMESTEP = metadata['n_window'], metadata['n_timestep']\n",
    "\n",
    "# FEATURES = metadata['feature_list']\n",
    "\n",
    "X_train, y_train = create_windowed_dataset(\n",
    "    train_df, FEATURES, 'Label', window_size=SLIDING_WINDOW_LENGTH, stride=SLIDING_WINDOW_STEP)\n",
    "X_val, y_val = create_windowed_dataset(\n",
    "    val_df, FEATURES, 'Label', window_size=SLIDING_WINDOW_LENGTH, stride=SLIDING_WINDOW_STEP)\n",
    "X_test, y_test = create_windowed_dataset(\n",
    "    test_df, FEATURES, 'Label', window_size=SLIDING_WINDOW_LENGTH, stride=SLIDING_WINDOW_STEP)\n",
    "\n",
    "# X_train = X_train.reshape(\n",
    "#     (X_train.shape[0], N_WINDOW, N_TIMESTEP, len(FEATURES)))\n",
    "# X_val = X_val.reshape(\n",
    "#     (X_val.shape[0], N_WINDOW, N_TIMESTEP, len(FEATURES)))\n",
    "# X_test = X_test.reshape(\n",
    "#     (X_test.shape[0], N_WINDOW, N_TIMESTEP, len(FEATURES)))\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=2)\n",
    "y_val = tf.keras.utils.to_categorical(y_val, num_classes=2)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9296, 320, 9)\n",
      "(9296, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_mid = np.repeat(np.expand_dims(y_train, axis=1), repeats=metadata['n_window'], axis=1)\n",
    "# y_val_mid = np.repeat(np.expand_dims(y_val, axis=1), repeats=metadata['n_window'], axis=1)\n",
    "# y_test_mid = np.repeat(np.expand_dims(y_test, axis=1), repeats=metadata['n_window'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_mid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWithContext(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "        Attention operation, with a context/query vector, for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "        \"Hierarchical Attention Networks for Document Classification\"\n",
    "        by using a context vector to assist the attention\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(AttentionWithContext())\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True,\n",
    "                 return_attention=False):\n",
    "        super(AttentionWithContext, self).__init__()\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        self.init = tf.keras.initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = tf.keras.regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = tf.keras.regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = tf.keras.regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = tf.keras.constraints.get(W_constraint)\n",
    "        self.u_constraint = tf.keras.constraints.get(u_constraint)\n",
    "        self.b_constraint = tf.keras.constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = tf.tensordot(x, self.W,axes=1)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = tf.keras.activations.tanh(uit)\n",
    "        ait = tf.tensordot(uit, self.u,axes=1)\n",
    "\n",
    "        a = tf.math.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= tf.cast(mask, tf.keras.backend.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= tf.cast(tf.keras.backend.sum(a, axis=1, keepdims=True) + tf.keras.backend.epsilon(), tf.keras.backend.floatx())\n",
    "\n",
    "        a = tf.keras.backend.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        result = tf.keras.backend.sum(weighted_input, axis=1)\n",
    "\n",
    "        if self.return_attention:\n",
    "            return [result, a]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_attention:\n",
    "            #TODO use TensorShape here, as done in the else statement.   I'm not sure\n",
    "            # if this is returning a single tensor, or a list of two so leaving this undone for now.  Suspect this will\n",
    "            # need to complete if using Sequential rather than Functional API\n",
    "            return [(input_shape[0], input_shape[-1]),\n",
    "                    (input_shape[0], input_shape[1])]\n",
    "        else:\n",
    "            return tf.TensorShape([input_shape[0].value,input_shape[-1].value])\n",
    "        \n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, position, d_model):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "  def get_angles(self, position, i, d_model):\n",
    "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "    return position * angles\n",
    "\n",
    "  def positional_encoding(self, position, d_model):\n",
    "    angle_rads = self.get_angles(\n",
    "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "        d_model=d_model)\n",
    "    # apply sin to even index in the array\n",
    "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "    # apply cos to odd index in the array\n",
    "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "  \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead) \n",
    "  but it must be broadcastable for addition.\n",
    "  \n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "    \n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "  \"\"\"\n",
    "\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "  # scale matmul_qk\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "  if mask is not None:\n",
    "    scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "  return output, attention_weights\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "    \n",
    "    assert d_model % self.num_heads == 0\n",
    "    \n",
    "    self.depth = d_model // self.num_heads\n",
    "    \n",
    "    self.wq = tf.keras.layers.Dense(d_model,use_bias=False)\n",
    "    self.wk = tf.keras.layers.Dense(d_model,use_bias=True)\n",
    "    self.wv = tf.keras.layers.Dense(d_model,use_bias=True)\n",
    "    \n",
    "    self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    \n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "    \n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "    \n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "    \n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "    return output, attention_weights\n",
    "\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  def call(self, x, training, mask=None):\n",
    "\n",
    "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "    attn_output = self.dropout1(attn_output, training=training)\n",
    "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "    ffn_output = self.dropout2(ffn_output, training=training)\n",
    "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    return out2\n",
    "\n",
    "class SensorAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_filters, kernel_size, dilation_rate):\n",
    "        super(SensorAttention, self).__init__()\n",
    "        self.conv_1 = tf.keras.layers.Conv2D(n_filters, kernel_size=kernel_size, \n",
    "                                                      dilation_rate=dilation_rate, padding='same', activation='relu')\n",
    "        self.conv_2 = tf.keras.layers.SeparableConv2D(n_filters, kernel_size=kernel_size, \n",
    "                                                      dilation_rate=dilation_rate*2, padding='same', activation='relu')\n",
    "        self.conv_f = tf.keras.layers.Conv2D(1, kernel_size=1, padding='same')\n",
    "        self.ln = tf.keras.layers.LayerNormalization()\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.ln(x)\n",
    "        x1 = tf.expand_dims(x, axis=3)\n",
    "        # print(x1.shape)\n",
    "        x1 = self.conv_1(x1)\n",
    "        # x1 = self.conv_2(x1)\n",
    "        x1 = self.conv_f(x1)\n",
    "        x1 = tf.keras.activations.softmax(x1, axis=2)\n",
    "        x1 = tf.keras.layers.Reshape(x.shape[-2:])(x1)\n",
    "        # print(x1.shape)\n",
    "        #print(x1.numpy)\n",
    "        return tf.math.multiply(x,x1), x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(n_timesteps, n_features, n_outputs,_dff = 512,d_model=128,nh = 4,dropout_rate=0.2,use_pe=True):\n",
    "    inputs = tf.keras.layers.Input(shape=(n_timesteps,n_features,))\n",
    "    si,_ = SensorAttention(n_filters=128, kernel_size =3, dilation_rate=2)(inputs)\n",
    "    x = tf.keras.layers.Conv1D(d_model, 1, activation='relu')(si)\n",
    "    if use_pe:\n",
    "        x *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "        x = PositionalEncoding(n_timesteps, d_model)(x)\n",
    "        x = tf.keras.layers.Dropout(rate=dropout_rate)(x)\n",
    "    x = EncoderLayer(d_model=d_model, num_heads=nh, dff=_dff, rate=dropout_rate) (x)\n",
    "    x = EncoderLayer(d_model=d_model, num_heads=nh, dff=_dff, rate=dropout_rate) (x)\n",
    "    #x = tf.keras.layers.Dense(128, activation='relu') (x)\n",
    "    x = AttentionWithContext() (x)\n",
    "    x = tf.keras.layers.Dense(n_outputs*4, activation='relu') (x)\n",
    "    x = tf.keras.layers.Dropout(0.2) (x)\n",
    "    predictions = tf.keras.layers.Dense(n_outputs, activation='softmax') (x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=predictions)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Physical GPUs, 8 Logical GPUs\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:5', '/job:localhost/replica:0/task:0/device:GPU:6', '/job:localhost/replica:0/task:0/device:GPU:7')\n",
      "Number of devices: 3\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 320, 9)]          0         \n",
      "_________________________________________________________________\n",
      "sensor_attention (SensorAtte ((None, 320, 9), (None, 3 1427      \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 320, 256)          2560      \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Mul (TensorFlowO [(None, 320, 256)]        0         \n",
      "_________________________________________________________________\n",
      "positional_encoding (Positio (None, 320, 256)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 320, 256)          0         \n",
      "_________________________________________________________________\n",
      "encoder_layer (EncoderLayer) (None, 320, 256)          789504    \n",
      "_________________________________________________________________\n",
      "encoder_layer_1 (EncoderLaye (None, 320, 256)          789504    \n",
      "_________________________________________________________________\n",
      "attention_with_context (Atte (None, 256)               66048     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 8)                 2056      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 2)                 18        \n",
      "=================================================================\n",
      "Total params: 1,651,117\n",
      "Trainable params: 1,651,117\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# print(gpus)\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "    \n",
    "device_list = ['/gpu:'+str(i) for i in range(5, 8)]\n",
    "strategy = tf.distribute.MirroredStrategy(devices=device_list)\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "with strategy.scope():\n",
    "    n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], y_train.shape[1]\n",
    "    model = get_model(n_timesteps, n_features, n_outputs,d_model=256,_dff = 1024,nh =4,dropout_rate=0.2)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "INFO:tensorflow:batch_all_reduce: 45 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 45 all-reduces with algorithm = nccl, num_packs = 1\n",
      "22/22 [==============================] - 16s 728ms/step - loss: 0.6667 - accuracy: 0.5989 - val_loss: 0.4726 - val_accuracy: 0.8161 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 13s 596ms/step - loss: 0.5372 - accuracy: 0.7260 - val_loss: 0.4377 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 13s 596ms/step - loss: 0.4395 - accuracy: 0.7637 - val_loss: 0.4721 - val_accuracy: 0.9043 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 13s 594ms/step - loss: 0.4039 - accuracy: 0.7704 - val_loss: 0.4128 - val_accuracy: 0.8774 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 13s 596ms/step - loss: 0.3916 - accuracy: 0.8152 - val_loss: 0.4894 - val_accuracy: 0.9043 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 13s 596ms/step - loss: 0.3623 - accuracy: 0.8194 - val_loss: 0.4794 - val_accuracy: 0.8925 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 13s 599ms/step - loss: 0.3613 - accuracy: 0.8129 - val_loss: 0.4679 - val_accuracy: 0.9065 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 13s 598ms/step - loss: 0.3483 - accuracy: 0.8182 - val_loss: 0.5784 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 13s 597ms/step - loss: 0.3408 - accuracy: 0.8284 - val_loss: 0.6348 - val_accuracy: 0.8613 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3299 - accuracy: 0.8377\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "22/22 [==============================] - 13s 598ms/step - loss: 0.3299 - accuracy: 0.8377 - val_loss: 0.9267 - val_accuracy: 0.6000 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f179c498c10>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbose, epochs, batch_size = 1, 10, 128\n",
    "# earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10,verbose=1, mode='max')\n",
    "#mcp_save = ModelCheckpoint('test_3_best.hdf5', save_best_only=True, monitor='val_acc', mode='max')\n",
    "reduce_lr_loss = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=6, verbose=1, min_delta=1e-4, mode='min')\n",
    "\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size*len(device_list),\n",
    "              verbose=verbose, validation_split=0.1,\n",
    "          callbacks=[reduce_lr_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 13s 592ms/step - loss: 0.3164 - accuracy: 0.8456 - val_loss: 0.7581 - val_accuracy: 0.7602\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f17085ad4f0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=1, batch_size=batch_size*len(device_list),\n",
    "              verbose=verbose, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sess = model.predict(X_test, batch_size=len(device_list) * 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_map = {1:'lying',2:'sitting',3:'standing',4:'walking',5:'running',6:'cycling',7:'Nordic walking',\n",
    "#               11:'ascending stairs',12:'descending stairs',13:'vacuum cleaning',\n",
    "#               14:'ironing', 18:'rope jumping'}\n",
    "# class_names = list(labels_map.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.75      0.80       443\n",
      "           1       0.80      0.88      0.84       490\n",
      "\n",
      "    accuracy                           0.82       933\n",
      "   macro avg       0.82      0.82      0.82       933\n",
      "weighted avg       0.82      0.82      0.82       933\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(np.argmax(y_test, axis=1), np.argmax(pred_sess, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.75      0.80       443\n",
      "           1       0.80      0.88      0.84       490\n",
      "\n",
      "    accuracy                           0.82       933\n",
      "   macro avg       0.82      0.82      0.82       933\n",
      "weighted avg       0.82      0.82      0.82       933\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(np.argmax(y_test, axis=1), np.argmax(pred_sess, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activity_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "confm = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(pred_sess, axis=1))\n",
    "# activity_list = list(activity_map.values())\n",
    "df_cm = pd.DataFrame(confm)\n",
    "plt.figure(figsize = (10,8))\n",
    "sns.heatmap(df_cm, annot=True, fmt='d', cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confm = confusion_matrix(np.argmax(y_test_mid.reshape(-1, 2), axis=1), np.argmax(pred_mid.reshape(-1, 2), axis=1))\n",
    "# activity_list = list(activity_map.values())\n",
    "df_cm = pd.DataFrame(confm)\n",
    "plt.figure(figsize = (10,8))\n",
    "sns.heatmap(df_cm, annot=True, fmt='d', cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
